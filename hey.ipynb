{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pchelkas/ShareFile_threads/blob/master/hey.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKl3yLuUNEj3",
        "colab_type": "code",
        "outputId": "e967dc7c-b5fb-4dfe-db72-ce37afc7b1fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from pandas import read_csv\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import brown\n",
        "import math\n",
        "import nltk\n",
        "import sys\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('brown')\n",
        "\n",
        "# In[3]:\n",
        "\n",
        "\n",
        "\n",
        "ALPHA = 0.2\n",
        "BETA = 0.45\n",
        "ETA = 0.4\n",
        "PHI = 0.2\n",
        "DELTA = 0.9\n",
        "\n",
        "brown_freqs = dict()\n",
        "N = 0\n",
        "\n",
        "\n",
        "# In[8]:\n",
        "\n",
        "######################### word similarity ##########################\n",
        "def get_best_synset_pair(word_1, word_2):\n",
        "    \n",
        "    max_sim = -1.0\n",
        "    synsets_1 = wn.synsets(word_1)\n",
        "    synsets_2 = wn.synsets(word_2)\n",
        "    if len(synsets_1) == 0 or len(synsets_2) == 0:\n",
        "        return None, None\n",
        "    else:\n",
        "        max_sim = -1.0\n",
        "        best_pair = None, None\n",
        "        for synset_1 in synsets_1:\n",
        "            for synset_2 in synsets_2:\n",
        "               sim = wn.path_similarity(synset_1, synset_2)\n",
        "               if sim!=None and sim > max_sim:\n",
        "                   max_sim = sim\n",
        "                   best_pair = synset_1, synset_2\n",
        "        return best_pair\n",
        "\n",
        "def length_dist(synset_1, synset_2):\n",
        "    \n",
        "    l_dist = sys.maxsize\n",
        "    if synset_1 is None or synset_2 is None: \n",
        "        return 0.0\n",
        "    if synset_1 == synset_2:\n",
        "        # if synset_1 and synset_2 are the same synset return 0\n",
        "        l_dist = 0.0\n",
        "    else:\n",
        "        wset_1 = set([str(x.name()) for x in synset_1.lemmas()])        \n",
        "        wset_2 = set([str(x.name()) for x in synset_2.lemmas()])\n",
        "        if len(wset_1.intersection(wset_2)) > 0:\n",
        "            # if synset_1 != synset_2 but there is word overlap, return 1.0\n",
        "            l_dist = 1.0\n",
        "        else:\n",
        "            # just compute the shortest path between the two\n",
        "            l_dist = synset_1.shortest_path_distance(synset_2)\n",
        "            if l_dist is None:\n",
        "                l_dist = 0.0\n",
        "    # normalize path length to the range [0,1]\n",
        "    return math.exp(-ALPHA * l_dist)\n",
        "\n",
        "def hierarchy_dist(synset_1, synset_2):\n",
        "    \n",
        "    h_dist = sys.maxsize\n",
        "    if synset_1 is None or synset_2 is None: \n",
        "        return h_dist\n",
        "    if synset_1 == synset_2:\n",
        "        # return the depth of one of synset_1 or synset_2\n",
        "        h_dist = max([x[1] for x in synset_1.hypernym_distances()])\n",
        "    else:\n",
        "        # find the max depth of least common subsumer\n",
        "        hypernyms_1 = {x[0]:x[1] for x in synset_1.hypernym_distances()}\n",
        "        hypernyms_2 = {x[0]:x[1] for x in synset_2.hypernym_distances()}\n",
        "        lcs_candidates = set(hypernyms_1.keys()).intersection(\n",
        "            set(hypernyms_2.keys()))\n",
        "        if len(lcs_candidates) > 0:\n",
        "            lcs_dists = []\n",
        "            for lcs_candidate in lcs_candidates:\n",
        "                lcs_d1 = 0\n",
        "                if lcs_candidate in hypernyms_1:\n",
        "                    lcs_d1 = hypernyms_1[lcs_candidate]\n",
        "                lcs_d2 = 0\n",
        "                if lcs_candidate in hypernyms_2:\n",
        "                    lcs_d2 = hypernyms_2[lcs_candidate]\n",
        "                lcs_dists.append(max([lcs_d1, lcs_d2]))\n",
        "            h_dist = max(lcs_dists)\n",
        "        else:\n",
        "            h_dist = 0\n",
        "    return ((math.exp(BETA * h_dist) - math.exp(-BETA * h_dist)) / \n",
        "        (math.exp(BETA * h_dist) + math.exp(-BETA * h_dist)))\n",
        "    \n",
        "def word_similarity(word_1, word_2):\n",
        "    synset_pair = get_best_synset_pair(word_1, word_2)\n",
        "    return (length_dist(synset_pair[0], synset_pair[1]) * \n",
        "        hierarchy_dist(synset_pair[0], synset_pair[1]))\n",
        "\n",
        "######################### sentence similarity ##########################\n",
        "\n",
        "def most_similar_word(word, word_set):\n",
        "    max_sim = -1.0\n",
        "    sim_word = \"\"\n",
        "    for ref_word in word_set:\n",
        "      sim = word_similarity(word, ref_word)\n",
        "      if sim > max_sim:\n",
        "          max_sim = sim\n",
        "          sim_word = ref_word\n",
        "    return sim_word, max_sim\n",
        "    \n",
        "def info_content(lookup_word):\n",
        "    \n",
        "    global N\n",
        "    if N == 0:\n",
        "        # poor man's lazy evaluation\n",
        "        for sent in brown.sents():\n",
        "            for word in sent:\n",
        "                word = word.lower()\n",
        "                if word not in brown_freqs:\n",
        "                    brown_freqs[word] = 0\n",
        "                brown_freqs[word] = brown_freqs[word] + 1\n",
        "                N = N + 1\n",
        "    lookup_word = lookup_word.lower()\n",
        "    n = 0 if lookup_word not in brown_freqs else brown_freqs[lookup_word]\n",
        "    return 1.0 - (math.log(n + 1) / math.log(N + 1))\n",
        "    \n",
        "def semantic_vector(words, joint_words, info_content_norm):\n",
        "    sent_set = set(words)\n",
        "    semvec = np.zeros(len(joint_words))\n",
        "    i = 0\n",
        "    for joint_word in joint_words:\n",
        "        if joint_word in sent_set:\n",
        "            # if word in union exists in the sentence, s(i) = 1 (unnormalized)\n",
        "            semvec[i] = 1.0\n",
        "            if info_content_norm:\n",
        "                semvec[i] = semvec[i] * math.pow(info_content(joint_word), 2)\n",
        "        else:\n",
        "            # find the most similar word in the joint set and set the sim value\n",
        "            sim_word, max_sim = most_similar_word(joint_word, sent_set)\n",
        "            semvec[i] = PHI if max_sim > PHI else 0.0\n",
        "            if info_content_norm:\n",
        "                semvec[i] = semvec[i] * info_content(joint_word) * info_content(sim_word)\n",
        "        i = i + 1\n",
        "    return semvec                \n",
        "            \n",
        "def semantic_similarity(s1,s2,t):\n",
        "    sentence_1 = re.sub('[^A-Za-z0-9\\s]', '', s1).lower()\n",
        "    sentence_2 = re.sub('[^A-Za-z0-9\\s]', '', s2).lower()\n",
        "    info_content_norm = t\n",
        "    words_1 = nltk.word_tokenize(sentence_1)\n",
        "    words_2 = nltk.word_tokenize(sentence_2)\n",
        "    joint_words = set(words_1).union(set(words_2))\n",
        "    vec_1 = semantic_vector(words_1, joint_words, info_content_norm)\n",
        "    vec_2 = semantic_vector(words_2, joint_words, info_content_norm)\n",
        "    return np.dot(vec_1, vec_2.T) / (np.linalg.norm(vec_1) * np.linalg.norm(vec_2))\n",
        "\n",
        "\n",
        "# In[9]:\n",
        "\n",
        "######################### word order similarity ##########################\n",
        "\n",
        "def word_order_vector(words, joint_words, windex):\n",
        "    wovec = np.zeros(len(joint_words))\n",
        "    i = 0\n",
        "    wordset = set(words)\n",
        "    for joint_word in joint_words:\n",
        "        if joint_word in wordset:\n",
        "            # word in joint_words found in sentence, just populate the index\n",
        "            wovec[i] = windex[joint_word]\n",
        "        else:\n",
        "            # word not in joint_words, find most similar word and populate\n",
        "            # word_vector with the thresholded similarity\n",
        "            sim_word, max_sim = most_similar_word(joint_word, wordset)\n",
        "            if max_sim > ETA:\n",
        "                wovec[i] = windex[sim_word]\n",
        "            else:\n",
        "                wovec[i] = 0\n",
        "        i = i + 1\n",
        "    return wovec\n",
        "\n",
        "def word_order_similarity(sentence_1, sentence_2):\n",
        "    words_1 = nltk.word_tokenize(sentence_1)\n",
        "    words_2 = nltk.word_tokenize(sentence_2)\n",
        "    joint_words = list(set(words_1).union(set(words_2)))\n",
        "    windex = {x[1]: x[0] for x in enumerate(joint_words)}\n",
        "    r1 = word_order_vector(words_1, joint_words, windex)\n",
        "    r2 = word_order_vector(words_2, joint_words, windex)\n",
        "    return 1.0 - (np.linalg.norm(r1 - r2) / np.linalg.norm(r1 + r2))\n",
        "\n",
        "\n",
        "# In[10]:\n",
        "\n",
        "def similarity(sentence_1, sentence_2, info_content_norm):\n",
        "    return DELTA * semantic_similarity(sentence_1, sentence_2, info_content_norm) +         (1.0 - DELTA) * word_order_similarity(sentence_1, sentence_2)\n",
        "\n",
        "\n",
        "# In[21]:\n",
        "\n",
        "######################### tf-idf measures ##########################\n",
        "\n",
        "def tfidf_vector_similarity(sentence_1, sentence_2):\n",
        "    corpus = [sentence_1, sentence_2]\n",
        "    vectorizer = TfidfVectorizer(min_df=1)\n",
        "    vec_1 = vectorizer.fit_transform(corpus).toarray()[0]\n",
        "    vec_2 = vectorizer.fit_transform(corpus).toarray()[1]\n",
        "    sim = np.dot(vec_1, vec_2.T) / (np.linalg.norm(vec_1) * np.linalg.norm(vec_2))\n",
        "    return sim\n",
        "\n",
        "\n",
        "# In[20]:\n",
        "\n",
        "######################### word overlap measures ##########################\n",
        "\n",
        "def jaccard_similarity_coefficient(sentence_1, sentence_2):\n",
        "    words_1 = nltk.word_tokenize(sentence_1)\n",
        "    words_2 = nltk.word_tokenize(sentence_2)\n",
        "    joint_words = set(words_1).union(set(words_2))\n",
        "    intersection_words = set(words_1).intersection(set(words_2))\n",
        "    return len(intersection_words)/len(joint_words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAuxOwe7Y5_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT_fYIfWOVi7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEmmdP2BOGe3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxE0WQCLN8FJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWW9Qve6NNNa",
        "colab_type": "code",
        "outputId": "5ba05a7f-a3c4-4817-9123-b1041b977b3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(similarity(\"Scene monkey try eat snow\", \"Primat monkey animal macaque\", True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3604363054920129\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3saMuuaHRM4E",
        "colab_type": "code",
        "outputId": "6f35cfd9-880e-48b7-c76a-301c9dea9b48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(similarity(\"Scene monkey try eat snow\", \"water wave rid surf water sport man sport surf \", True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3043345022006478\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2Jzw0zRQ8e8",
        "colab_type": "code",
        "outputId": "1a56d522-be92-49c2-8336-72eba71a51ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(similarity(\"Scene monkey try eat snow\", \"road person practice street motion winter autumn \", True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.27297971073227956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX8S49xqQxfY",
        "colab_type": "code",
        "outputId": "c8b767ce-e99c-4efc-f0f4-ddcc3b915579",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(similarity(\"Scene monkey try eat snow\", \"surf sky beach\", True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.14419282628808203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H_eeh68Tb31",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9PLgPXuQt8B",
        "colab_type": "code",
        "outputId": "7b7a0e06-9d29-4a0f-b553-8e3c6c7a25a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(similarity(\"Scene monkey try eat snow\", \"laser scene night dark\", True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.34587911351295275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEKrWOLaQqRz",
        "colab_type": "code",
        "outputId": "5f06cf32-2437-4384-8f58-e3cd7b2c86f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(similarity(\"people know panther look life\", \"sky invertebrate plant\", True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.15672891905496175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDd0fdCQT9Zm",
        "colab_type": "code",
        "outputId": "816145da-6d10-4489-f1a1-f6392f80ef83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(similarity(\"people know panther look life\", \"person weapon gun presentation clothing \", True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.22775459497672645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOF14BxhUBas",
        "colab_type": "code",
        "outputId": "8b6a80ab-1aaf-47ac-c69e-fcc1f4ee8c85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(similarity(\"people know panther look life\", \"person street  \", True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.1894551080909431\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GSEaglLUOIZ",
        "colab_type": "code",
        "outputId": "d5970654-cc50-4db4-d34a-9b55df0e024c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(similarity(\"people know panther look life\", \"sky water nature sunset \", True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.18770271017364407\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8iE447SUTxO",
        "colab_type": "code",
        "outputId": "485f9833-6a06-4225-cbb6-fa68fdb44e9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(similarity(\"people know panther look life\", \"animal cat cat leopard zoo jaguar \", True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.35665878918533983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycIuaMdFU8fX",
        "colab_type": "code",
        "outputId": "58c64da6-d7b5-4c8e-ce25-8d3cfe092899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(similarity(\"animal\", \"brick building material  \", True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3295183830474203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZiS8jAuVSif",
        "colab_type": "code",
        "outputId": "2272b902-6c04-42f5-c1d5-1190cdc09b51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(similarity(\"animal\", \"ground animal dog  \", True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6525681654471921\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXNk-V8SVZMp",
        "colab_type": "code",
        "outputId": "da04ccb6-8a06-4a67-e042-71f3979cf7bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(similarity(\"animal\", \"accessory \", True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3603167989474042\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R63jfg3VbRd",
        "colab_type": "code",
        "outputId": "1eccb73e-1fe2-430d-cf96-def49ecddac4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(similarity(\"animal may find see goat feed\",\n",
        "\"animal person sheep pet goat \", True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6632108806755111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvw2IIy1WNy6",
        "colab_type": "code",
        "outputId": "39cc3293-5c9b-4089-b0a1-6b430d5eada3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(similarity(\"animal may find see goat feed\",\n",
        "\"animal grass stand look sheep  \", True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4905360465311601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kazyQhe_WTHR",
        "colab_type": "code",
        "outputId": "8ddf84bb-e7db-40f7-f607-5e85e1bf94b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(similarity(\"animal may find see goat feed\",\n",
        "\"grass sheep rock animal stand field hillside stone  \", True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4719654128843711\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQMGDfwZWXzO",
        "colab_type": "code",
        "outputId": "29032a2b-e741-4515-a10c-dcb869403ce4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(similarity(\"animal may find see goat feed\",\n",
        "\"animal standing goat \", True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.775688498747424\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AW86B5XWg6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz9EDFYEWdXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udNnu2UsVW_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDpLgjehUGk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYrzPEHBQlrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqjpadnkPxPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v49GV7LPn9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU54weKfN0WV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9hrGuIuNgD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC9eCyGPNdem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx6NpLlMNZdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}